{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7719145,"sourceType":"datasetVersion","datasetId":4508630}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# Function to load images and preprocess data\ndef load_data(data_dir, image_height, image_width):\n    images = []\n    tread_depths = []\n\n    for subdir, _, files in os.walk(data_dir):\n        for file in files:\n            if file.endswith('.jpg'):\n                # Read image\n                image_path = os.path.join(subdir, file)\n                image = cv2.imread(image_path)\n                image = cv2.resize(image, (image_width, image_height))\n                images.append(image)\n                \n                # Extract tread depth from filename or metadata\n                tread_depth_str = ''.join(filter(str.isdigit, file))  # Extract only numerical characters\n                tread_depth = float(tread_depth_str)  # Convert to float\n                tread_depths.append(tread_depth)\n\n    # Convert lists to numpy arrays\n    images = np.array(images)\n    tread_depths = np.array(tread_depths)\n\n    return images, tread_depths\n\n# Load data\ndata_dir = '/kaggle/input/data707'  # Replace with the path to your dataset directory\nimage_height, image_width = 100, 100  # Specify image dimensions\nimages, tread_depths = load_data(data_dir, image_height, image_width)\n\n# Split data into training and validation sets\ntrain_images, val_images, train_tread_depths, val_tread_depths = train_test_split(\n    images, tread_depths, test_size=0.2, random_state=42)\n\n# Define the model architecture\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)  # Output layer for tread depth prediction\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='mean_squared_error',  # Use MSE loss for regression\n              metrics=['mae'])  # Mean Absolute Error as evaluation metric\n\n# Train the model\nhistory = model.fit(train_images, train_tread_depths,\n                    epochs=10,\n                    validation_data=(val_images, val_tread_depths))\n\n# Evaluate the model\nval_loss, val_mae = model.evaluate(val_images, val_tread_depths)\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation MAE:\", val_mae)\n\n# Save the model\nmodel.save('tread_depth_model.h5')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-28T09:40:02.850545Z","iopub.execute_input":"2024-02-28T09:40:02.850987Z","iopub.status.idle":"2024-02-28T09:40:06.745145Z","shell.execute_reply.started":"2024-02-28T09:40:02.850954Z","shell.execute_reply":"2024-02-28T09:40:06.743923Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 3427.8572 - mae: 43.9179 - val_loss: 209355.2344 - val_mae: 457.5535\nEpoch 2/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - loss: 276963.1562 - mae: 517.8609 - val_loss: 6992.4517 - val_mae: 83.6209\nEpoch 3/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 2941.7275 - mae: 45.6563 - val_loss: 19415.0566 - val_mae: 139.3379\nEpoch 4/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 10121.8535 - mae: 95.8539 - val_loss: 12098.4395 - val_mae: 109.9929\nEpoch 5/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 5565.3965 - mae: 67.5749 - val_loss: 6931.5825 - val_mae: 83.2561\nEpoch 6/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 2772.0996 - mae: 41.6505 - val_loss: 3692.0073 - val_mae: 60.7619\nEpoch 7/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1470.0422 - mae: 34.8290 - val_loss: 1845.0208 - val_mae: 42.9537\nEpoch 8/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1040.1826 - mae: 32.1028 - val_loss: 1774.7152 - val_mae: 42.1274\nEpoch 9/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 554.5971 - mae: 23.4023 - val_loss: 5707.6279 - val_mae: 75.5488\nEpoch 10/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1077.4075 - mae: 31.9813 - val_loss: 2.1305 - val_mae: 1.4596\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.1305 - mae: 1.4596\nValidation Loss: 2.1304612159729004\nValidation MAE: 1.4596099853515625\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model\nval_loss, val_mae = model.evaluate(val_images, val_tread_depths)\nprint(\"Validation Loss:\", val_loss)\nprint(\"Validation MAE:\", val_mae)\n\n# Calculate accuracy (lower MAE is better)\naccuracy = 1 - (val_mae / np.mean(val_tread_depths))\nprint(\"Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T09:41:02.437819Z","iopub.execute_input":"2024-02-28T09:41:02.438329Z","iopub.status.idle":"2024-02-28T09:41:02.538477Z","shell.execute_reply.started":"2024-02-28T09:41:02.438292Z","shell.execute_reply":"2024-02-28T09:41:02.537179Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.1305 - mae: 1.4596\nValidation Loss: 2.1304612159729004\nValidation MAE: 1.4596099853515625\nAccuracy: 0.9828281178193934\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}